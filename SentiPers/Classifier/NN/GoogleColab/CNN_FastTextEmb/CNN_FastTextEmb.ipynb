{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_FastTextEmb.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/parsa-abbasi/Basic-Sentiment-Analysis/blob/master/SentiPers/Classifier/NN/GoogleColab/CNN_FastText/CNN_FastText.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "QthdEc8Cp5Nt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Get FastText Persian"
      ]
    },
    {
      "metadata": {
        "id": "4wCOcr1ao1VX",
        "colab_type": "code",
        "outputId": "996797fd-5606-42ba-d4cc-9496f288c54c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fa.vec"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-21 17:21:12--  https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fa.vec\n",
            "Resolving s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)... 52.219.112.24\n",
            "Connecting to s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)|52.219.112.24|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1105157170 (1.0G) [binary/octet-stream]\n",
            "Saving to: ‘wiki.fa.vec’\n",
            "\n",
            "wiki.fa.vec         100%[===================>]   1.03G  73.5MB/s    in 14s     \n",
            "\n",
            "2019-01-21 17:21:27 (73.6 MB/s) - ‘wiki.fa.vec’ saved [1105157170/1105157170]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0yHgWvqWBAqS",
        "colab_type": "code",
        "outputId": "c3af6a0a-94e6-4123-d242-9b0fe4b6bd45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install hazm\n",
        "!pip install stopwords_guilannlp"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hazm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/13/5a7074bc11d20dbbb46239349ac3f85f7edc148b4cf68e9b8c2f8263830c/hazm-0.7.0-py3-none-any.whl (316kB)\n",
            "\u001b[K    100% |████████████████████████████████| 317kB 26.3MB/s \n",
            "\u001b[?25hCollecting nltk==3.3 (from hazm)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.4MB 19.5MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1; platform_system != \"Windows\" (from hazm)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/0f/1c9b49bb49821b5856a64ea6fac8d96a619b9f291d1f06999ea98a32c89c/libwapiti-0.2.1.tar.gz (233kB)\n",
            "\u001b[K    100% |████████████████████████████████| 235kB 28.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.3->hazm) (1.11.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Running setup.py bdist_wheel for nltk ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n",
            "  Running setup.py bdist_wheel for libwapiti ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/66/15/54/4510dce8bb958b1cdd2c47425cbd1e1eecc0480ac9bb1fb9ab\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n",
            "Collecting stopwords_guilannlp\n",
            "  Downloading https://files.pythonhosted.org/packages/44/bc/a01c003b59a91187e89d11e73e8bb2834bb9ae6b36fe576a4b617c90bd23/stopwords_guilannlp-13.2019.3.5-py3-none-any.whl\n",
            "Installing collected packages: stopwords-guilannlp\n",
            "Successfully installed stopwords-guilannlp-13.2019.3.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qHENRyUpp-fb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "metadata": {
        "id": "DyJ0hqR8nku1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6fe9dbd1-3261-44ab-871d-8032222334d3"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Convolution1D, GlobalMaxPooling1D\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.models import Model\n",
        "from keras import optimizers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.metrics import categorical_accuracy\n",
        "from keras.utils import plot_model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from gensim.models import KeyedVectors\n",
        "import codecs\n",
        "from stopwords_guilannlp import stopwords_output\n",
        "from hazm import *"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "56Zm6edfuwXC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# File uploader"
      ]
    },
    {
      "metadata": {
        "id": "VvTYlRzbuwcd",
        "colab_type": "code",
        "outputId": "49cd1c4d-e290-4fd0-ffdc-6ef7d978a655",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7876fddd-2dec-4d6a-8a5d-9454d8be4508\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-7876fddd-2dec-4d6a-8a5d-9454d8be4508\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving vocab.txt to vocab.txt\n",
            "Saving x_test.csv to x_test.csv\n",
            "Saving x_train.csv to x_train.csv\n",
            "Saving y_test.csv to y_test.csv\n",
            "Saving y_train.csv to y_train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6FbuK-Gmupe2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Import Dataset"
      ]
    },
    {
      "metadata": {
        "id": "RXEXLFZoupUr",
        "colab_type": "code",
        "outputId": "610ccaf6-ec49-409c-afd0-6422c8cae8d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "x_train = pd.Series.from_csv('x_train.csv', sep='\\t')\n",
        "x_test = pd.Series.from_csv('x_test.csv', sep='\\t')\n",
        "y_train = pd.Series.from_csv('y_train.csv', sep='\\t', header=0)\n",
        "y_test = pd.Series.from_csv('y_test.csv', sep='\\t', header=0)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:2890: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
            "  infer_datetime_format=infer_datetime_format)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "HMIMBSUa97Zm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train = x_train.iloc[1:, ]\n",
        "x_test = x_test.iloc[1:, ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OgJCr-PFx9d7",
        "colab_type": "code",
        "outputId": "5387e499-e5b9-418b-f014-a071c757acf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "print('x_train shape: ', x_train.shape)\n",
        "print('x_test shape: ', x_test.shape)\n",
        "print('y_train shape: ', y_train.shape)\n",
        "print('y_test shape: ', y_test.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape:  (5561,)\n",
            "x_test shape:  (1854,)\n",
            "y_train shape:  (5561,)\n",
            "y_test shape:  (1854,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lm9Jh2reugBZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Import Vocabulary"
      ]
    },
    {
      "metadata": {
        "id": "1Oe4eWeHr-fY",
        "colab_type": "code",
        "outputId": "2391e901-d041-4182-f00c-751d89129576",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def load_doc(filename):\n",
        "    file = codecs.open(filename, 'r', \"utf8\")\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "print('The size of vocab : ', len(vocab))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The size of vocab :  2671\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S-YylKUoqElV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Make FastText Model"
      ]
    },
    {
      "metadata": {
        "id": "1F_olE5An-5I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EMBEDDING_FILE = 'wiki.fa.vec'\n",
        "\n",
        "def import_with_gensim(file_address):\n",
        "    # Creating the model\n",
        "    ft_model = KeyedVectors.load_word2vec_format(file_address)\n",
        "    # Getting the tokens\n",
        "    ft_words = []\n",
        "    for ft_word in ft_model.vocab:\n",
        "        ft_words.append(ft_word)\n",
        "\n",
        "    return ft_model, ft_words\n",
        "  \n",
        "model, words = import_with_gensim(EMBEDDING_FILE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WbXOKymz-OS1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embed_size = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0lraPVp4ptF1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We get the mean and standard deviation of the embedding weights so that we could maintain the\n",
        "# same statistics for the rest of our own random generated weights.\\\n",
        "embedding_list = list()\n",
        "for w in words:\n",
        "    embedding_list.append(model[w])\n",
        "\n",
        "all_embedding = np.stack(embedding_list)\n",
        "emb_mean, emb_std = all_embedding.mean(), all_embedding.std()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aCPR-DfhyUWi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stop_set = stopwords_output(\"Persian\", \"set\")\n",
        "stop_useful = ['سلام', 'دیجی', 'کالا']\n",
        "useful_set = ['خوب', 'بد', 'کاملا', 'کاملاً', 'بسیار', 'واقعا', 'واقعاً', 'فوق', 'بخش', 'طرفدارترین', 'نیست', 'هست']\n",
        "puncs = ['،', '.', ',', ':', ';']\n",
        "\n",
        "for word in stop_useful:\n",
        "  if word not in stop_set:\n",
        "    stop_set.add(word)\n",
        "\n",
        "for word in useful_set:\n",
        "  if word in stop_set:\n",
        "    stop_set.remove(word)\n",
        "  if word not in vocab:\n",
        "    vocab.add(word)\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc, vocabulary):\n",
        "    tokenized = word_tokenize(doc)  # Tokenize text\n",
        "    tokens = []\n",
        "    for t in tokenized:\n",
        "      temp = t\n",
        "      for p in puncs:\n",
        "        temp = temp.replace(p, '')\n",
        "      tokens.append(temp)\n",
        "    tokens = [w for w in tokens if not w in stop_set]    # Remove stop words\n",
        "    tokens = [w for w in tokens if not len(w) <= 1]\n",
        "    tokens = [w for w in tokens if not w.isdigit()]\n",
        "    tokens = [w for w in tokens if w in vocabulary]\n",
        "    tokens = ' '.join(tokens)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "train_docs = list()\n",
        "for document in x_train:\n",
        "    train_docs.append(clean_doc(document, vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yXOfMkE1yMOG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_words = 2500\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "\n",
        "# fit the tokenizer on the documents\n",
        "tokenizer.fit_on_texts(train_docs)\n",
        "\n",
        "# sequence encode\n",
        "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
        "\n",
        "# pad sequences\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "x_train = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "\n",
        "test_docs = list()\n",
        "for document in x_test:\n",
        "    test_docs.append(clean_doc(document, vocab))\n",
        "\n",
        "# define vocabulary size (largest integer value)\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WZnIkLd4rUrb",
        "colab_type": "code",
        "outputId": "0bdf19f5-fb9f-4df5-ad97-966e59ad02b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# We are going to set the embedding size to the pre-trained dimension as we are replicating it\n",
        "nb_words = len(tokenizer.word_index)\n",
        "\n",
        "# the size will be Number of Words in Vocab X Embedding Size\n",
        "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
        "\n",
        "# With the newly created embedding matrix, we'll fill it up with the words that we have in both\n",
        "# our own dictionary and loaded pre-trained embedding.\n",
        "embeddedCount = 0\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    i -= 1\n",
        "    # then we see if this word is in glove's dictionary, if yes, get the corresponding weights\n",
        "    if word in model.vocab:\n",
        "        embedding_vector = model[word]\n",
        "        # and store inside the embedding matrix that we will train later on.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        embeddedCount += 1\n",
        "    else:   # Unknown words\n",
        "        embedding_vector = model['subdivision_name']\n",
        "        # and store inside the embedding matrix that we will train later on.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        embeddedCount += 1\n",
        "\n",
        "print('total embedded:', embeddedCount, 'common words')\n",
        "print('Embedding matrix shape:', embedding_matrix.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total embedded: 2587 common words\n",
            "Embedding matrix shape: (2587, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o8NcUFxQpN1f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
        "x_test = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UGzX4V3J2U9F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CNN Model"
      ]
    },
    {
      "metadata": {
        "id": "FBLkmoE6ZUIk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "categorical_y_train = to_categorical(y_train, 5)\n",
        "categorical_y_test = to_categorical(y_test, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ITMRqJHnAFCf",
        "colab_type": "code",
        "outputId": "474f3852-ff84-431c-dbe6-40c77f7b178f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        }
      },
      "cell_type": "code",
      "source": [
        "inp = Input(shape=(max_length, ))\n",
        "x = Embedding(len(tokenizer.word_index), embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False)(inp)\n",
        "x = Conv1D(filters=64, kernel_size=4, activation='relu', padding='same')(x)\n",
        "x = MaxPooling1D(pool_size=2)(x)\n",
        "x = Conv1D(filters=64, kernel_size=8, activation='relu', padding='same')(x)\n",
        "x = MaxPooling1D(pool_size=2)(x)\n",
        "x = Conv1D(filters=64, kernel_size=16, activation='relu', padding='same')(x)\n",
        "x = GlobalMaxPooling1D()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(500, activation=\"sigmoid\")(x)\n",
        "x = Dense(5, activation=\"softmax\")(x)\n",
        "\n",
        "# optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
        "# optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
        "# optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
        "# optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "\n",
        "opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "\n",
        "model = Model(inputs=inp, outputs=x)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=[categorical_accuracy])\n",
        "\n",
        "model.summary()\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "hist = model.fit(x_train, categorical_y_train, batch_size=batch_size, epochs=epochs)\n",
        "\n",
        "loss, acc = model.evaluate(x_test, categorical_y_test, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_18 (InputLayer)        (None, 181)               0         \n",
            "_________________________________________________________________\n",
            "embedding_18 (Embedding)     (None, 181, 300)          776100    \n",
            "_________________________________________________________________\n",
            "conv1d_22 (Conv1D)           (None, 181, 64)           76864     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_19 (MaxPooling (None, 90, 64)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_23 (Conv1D)           (None, 90, 64)            32832     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_20 (MaxPooling (None, 45, 64)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_24 (Conv1D)           (None, 45, 64)            65600     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_2 (Glob (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 500)               32500     \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 5)                 2505      \n",
            "=================================================================\n",
            "Total params: 986,401\n",
            "Trainable params: 210,301\n",
            "Non-trainable params: 776,100\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "5561/5561 [==============================] - 3s 497us/step - loss: 1.2887 - categorical_accuracy: 0.4334\n",
            "Epoch 2/10\n",
            "5561/5561 [==============================] - 1s 228us/step - loss: 1.2421 - categorical_accuracy: 0.4645\n",
            "Epoch 3/10\n",
            "5561/5561 [==============================] - 1s 227us/step - loss: 1.1189 - categorical_accuracy: 0.5364\n",
            "Epoch 4/10\n",
            "5561/5561 [==============================] - 1s 227us/step - loss: 1.0269 - categorical_accuracy: 0.5886\n",
            "Epoch 5/10\n",
            "5561/5561 [==============================] - 1s 228us/step - loss: 0.9446 - categorical_accuracy: 0.6339\n",
            "Epoch 6/10\n",
            "5561/5561 [==============================] - 1s 225us/step - loss: 0.8558 - categorical_accuracy: 0.6848\n",
            "Epoch 7/10\n",
            "5561/5561 [==============================] - 1s 232us/step - loss: 0.7575 - categorical_accuracy: 0.7285\n",
            "Epoch 8/10\n",
            "5561/5561 [==============================] - 1s 231us/step - loss: 0.6600 - categorical_accuracy: 0.7689\n",
            "Epoch 9/10\n",
            "5561/5561 [==============================] - 1s 227us/step - loss: 0.5598 - categorical_accuracy: 0.8072\n",
            "Epoch 10/10\n",
            "5561/5561 [==============================] - 1s 226us/step - loss: 0.4930 - categorical_accuracy: 0.8299\n",
            "Test Accuracy: 53.829558\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "I6Y5JQ--clHL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}